{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZPWtNYJZ65i"
      },
      "source": [
        "# Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiYW4IT1aCJX"
      },
      "outputs": [],
      "source": [
        "%pip install ipykernel pandas numpy faiss-cpu openai python-dotenv lxml beautifulsoup4 sentence-transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHYAa-vRbTio"
      },
      "outputs": [],
      "source": [
        "%pip install kagglehub[pandas-datasets]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctOED2JiaV4P"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVT_eOVQaZLm",
        "outputId": "0aa0538d-102f-41a4-ec38-c5b1993be3e0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from typing import List, Dict, Tuple\n",
        "from dotenv import load_dotenv\n",
        "import re\n",
        "\n",
        "# Vector store and embeddings\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# LLM\n",
        "from openai import OpenAI\n",
        "\n",
        "# Kaggle\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "load_dotenv()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOt9A5AJak4P"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qRgiGIXjagtn"
      },
      "outputs": [],
      "source": [
        "CHUNK_SIZE = 300  # Words per chunk\n",
        "TOP_K = 3  # Number of chunks to retrieve\n",
        "SAMPLE_SIZE = 500  # Number of movies to use (adjust for your needs)\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') # os.getenv(\"OPENAI_API_KEY\") use this to get data from .env\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKB4mCr6bI1W"
      },
      "source": [
        "# Load & Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bh0D44Ja-En",
        "outputId": "1f07a9fb-ab95-4a4e-ef92-97cf610bdae3"
      },
      "outputs": [],
      "source": [
        "print(\"Loading movie plots dataset...\")\n",
        "\n",
        "path = kagglehub.dataset_download(\"jrobischon/wikipedia-movie-plots\")\n",
        "file_path = os.path.join(path, \"wiki_movie_plots_deduped.csv\")\n",
        "\n",
        "df = pd.read_csv(file_path, encoding='latin-1', on_bad_lines='skip', engine='python')\n",
        "\n",
        "# Handle missing values\n",
        "df = df.dropna(subset=['Title', 'Plot'])\n",
        "\n",
        "# Sample data if needed\n",
        "df = df.sample(n=min(SAMPLE_SIZE, len(df)), random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(f\"Loaded {len(df)} movies\")\n",
        "print(f\"Sample titles: {df['Title'].head(3).tolist()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEU1ZJ0ccdp9"
      },
      "source": [
        "# Chunking Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2QsS2VqbK_W",
        "outputId": "43d60d1d-f41a-4b33-b964-4a31a944e7e1"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text: str, chunk_size: int = CHUNK_SIZE) -> List[str]:\n",
        "    \"\"\"Split text into chunks by word count.\"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), chunk_size):\n",
        "        chunk = ' '.join(words[i:i + chunk_size])\n",
        "        if chunk.strip():\n",
        "            chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "# Create chunks with metadata\n",
        "chunks_list = []\n",
        "chunk_metadata = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    title = row['Title']\n",
        "    plot = row['Plot']\n",
        "    chunks = chunk_text(plot)\n",
        "\n",
        "    for chunk_idx, chunk in enumerate(chunks):\n",
        "        chunks_list.append(chunk)\n",
        "        chunk_metadata.append({\n",
        "            'title': title,\n",
        "            'chunk_idx': chunk_idx,\n",
        "            'movie_idx': idx\n",
        "        })\n",
        "\n",
        "print(f\"Total chunks created: {len(chunks_list)}\")\n",
        "print(f\"Sample chunk: {chunks_list[0][:150]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61ILeJAtcmCG"
      },
      "source": [
        "# Embed & Store with Sentence Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "eed46a2a00b048658931515f7200ba57",
            "33b7aab67ae6444e9736c3af3e0e2353",
            "875bf8d032ee4a3ba77ca0dbba689181",
            "15d66ce6f9034bb0b195fee9b3c4a263",
            "35c38884c3bc418999307f191d7e9384",
            "01a9f9768b5d4601845d002fc42d7c29",
            "cc749e91e9c340a1a1f0a42bef7ff8f9",
            "2aed01c77a3646dd922c5e30806dd0ff",
            "ad3069c979834ebcaa578fb1909a29b0",
            "a0946a9a2a8840a99eaba297060f78ce",
            "d37bba5697cb4752b002a6701e9dc627"
          ]
        },
        "id": "MPAfGxoXcgiR",
        "outputId": "923b692e-23a7-4644-d84a-7b1fa9d49e10"
      },
      "outputs": [],
      "source": [
        "print(\"\\nCreating embeddings with Sentence Transformers...\")\n",
        "\n",
        "# Load pre-trained model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')  # Fast & lightweight\n",
        "\n",
        "# Encode all chunks\n",
        "embeddings = model.encode(chunks_list, show_progress_bar=True)\n",
        "embeddings = np.array(embeddings).astype('float32')\n",
        "\n",
        "print(f\"Embedding shape: {embeddings.shape}\")\n",
        "print(f\"Embedding dimension: {embeddings.shape[1]}\")\n",
        "\n",
        "# Store in FAISS with cosine similarity (better for embeddings)\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity\n",
        "faiss.normalize_L2(embeddings)\n",
        "index.add(embeddings)\n",
        "\n",
        "print(f\"FAISS index created with {index.ntotal} vectors\")\n",
        "\n",
        "# Store model for later queries\n",
        "model_for_queries = model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ECQDE8Dd-yv"
      },
      "source": [
        "# Retrieval Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UAjs_OBoct0s"
      },
      "outputs": [],
      "source": [
        "def retrieve_chunks(query: str, top_k: int = TOP_K) -> Dict:\n",
        "    \"\"\"Retrieve with Sentence Transformers.\"\"\"\n",
        "\n",
        "    # Embed query\n",
        "    query_embedding = model_for_queries.encode([query])[0]\n",
        "    query_embedding = np.array([query_embedding]).astype('float32')\n",
        "    faiss.normalize_L2(query_embedding)\n",
        "\n",
        "    # Search\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    results = {\n",
        "        'chunks': [chunks_list[i] for i in indices[0]],\n",
        "        'metadata': [chunk_metadata[i] for i in indices[0]],\n",
        "        'distances': distances[0].tolist(),  # Now similarity scores (higher = better)\n",
        "        'indices': indices[0].tolist()\n",
        "    }\n",
        "\n",
        "    results['retrieval_quality'] = evaluate_retrieval(\n",
        "        results['chunks'],\n",
        "        query,\n",
        "        results['distances']\n",
        "    )\n",
        "\n",
        "    return results\n",
        "\n",
        "def evaluate_retrieval(chunks: List[str], query: str, distances: List[float]) -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluate retrieval quality.\n",
        "\n",
        "    Metrics:\n",
        "    - proximity_score: Lower distance = better (0-1 scale)\n",
        "    - keyword_coverage: % of query words in retrieved chunks\n",
        "    - chunk_diversity: Are chunks from different movies?\n",
        "    \"\"\"\n",
        "    # Proximity score (inverse of L2 distance, normalized)\n",
        "    avg_distance = np.mean(distances)\n",
        "    proximity_score = 1 / (1 + avg_distance)  # Sigmoid-like transformation\n",
        "\n",
        "    # Keyword coverage\n",
        "    query_words = set(re.findall(r'\\w+', query.lower()))\n",
        "    chunk_text = ' '.join(chunks).lower()\n",
        "    covered_words = query_words & set(re.findall(r'\\w+', chunk_text))\n",
        "    keyword_coverage = len(covered_words) / len(query_words) if query_words else 0\n",
        "\n",
        "    # Overall retrieval score\n",
        "    overall_score = (proximity_score * 0.4 + keyword_coverage * 0.6)\n",
        "\n",
        "    return {\n",
        "        'proximity_score': round(proximity_score, 3),\n",
        "        'keyword_coverage': round(keyword_coverage, 3),\n",
        "        'overall_retrieval_score': round(overall_score, 3),\n",
        "        'quality_label': 'Good' if overall_score > 0.6 else 'Fair' if overall_score > 0.4 else 'Poor'\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WX_YV5eeV9M"
      },
      "source": [
        "# LLM Answer Generation with Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "j_FwGMFXeAS2"
      },
      "outputs": [],
      "source": [
        "def generate_answer(query: str, contexts: List[str]) -> Dict:\n",
        "    \"\"\"\n",
        "    Generate answer using LLM with quality evaluation.\n",
        "    \"\"\"\n",
        "    context_text = \"\\n\\n\".join([f\"[Context {i+1}]: {ctx}\" for i, ctx in enumerate(contexts)])\n",
        "\n",
        "    prompt = f\"\"\"Based on the following movie plot contexts, answer the question concisely and accurately.\n",
        "\n",
        "    Question: {query}\n",
        "\n",
        "    Contexts:\n",
        "    {context_text}\n",
        "\n",
        "    Provide a clear, factual answer. If the contexts don't contain relevant information, say so.\n",
        "    Output Format:\n",
        "        Responses must be valid JSON per RFC8259. Do not change keys or structure. Format:\n",
        "            \"answer\": \"natural language answer\"\n",
        "            \"reasoning\": \"short explanation of how the answer was formed, add movie name as reference, do not add context number\"\n",
        "\n",
        "    Example Output\n",
        "    {{\n",
        "      \"answer\": \"The movie *2001: A Space Odyssey* features an artificial intelligence system called HAL 9000.\",\n",
        "      \"reasoning\": \"The question asked about AI. I searched the plots, found '2001: A Space Odyssey' with HAL 9000, and used it to form the answer.\"\n",
        "    }}\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=500,\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    result = response.choices[0].message.content\n",
        "    clean_result = result.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
        "    json_result = json.loads(clean_result)\n",
        "\n",
        "    # Evaluate answer quality\n",
        "    quality = evaluate_answer(json_result['answer'], contexts, query)\n",
        "\n",
        "    return {\n",
        "        'answer': json_result['answer'],\n",
        "        'reasoning' : json_result['reasoning'],\n",
        "        'answer_quality': quality\n",
        "    }\n",
        "\n",
        "def evaluate_answer(answer: str, contexts: List[str], query: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluate answer quality using heuristics.\n",
        "\n",
        "    Metrics:\n",
        "    - grounding: Does answer reference the contexts?\n",
        "    - length_appropriateness: Is answer concise but complete?\n",
        "    - relevance: Does answer address the query?\n",
        "    - confidence: Presence of hedging language\n",
        "    \"\"\"\n",
        "    context_combined = ' '.join(contexts).lower()\n",
        "    answer_lower = answer.lower()\n",
        "\n",
        "    # 1. Grounding: Check if answer uses words from contexts\n",
        "    answer_words = set(re.findall(r'\\w+', answer_lower))\n",
        "    context_words = set(re.findall(r'\\w+', context_combined))\n",
        "    grounding_score = len(answer_words & context_words) / len(answer_words) if answer_words else 0\n",
        "\n",
        "    # 2. Length appropriateness (100-300 chars is good)\n",
        "    length_score = 1.0 if 50 <= len(answer) <= 500 else 0.5\n",
        "\n",
        "    # 3. Relevance: Check if query words are in answer\n",
        "    query_words = set(re.findall(r'\\w+', query.lower()))\n",
        "    relevance_score = len(query_words & answer_words) / len(query_words) if query_words else 0\n",
        "\n",
        "    # 4. Confidence: Check for hedging language\n",
        "    hedging_words = ['might', 'maybe', 'possibly', 'unclear', 'not found', 'no information']\n",
        "    has_hedging = any(word in answer_lower for word in hedging_words)\n",
        "    confidence_score = 0.7 if has_hedging else 0.95\n",
        "\n",
        "    # Overall score\n",
        "    overall = (grounding_score * 0.3 + length_score * 0.2 +\n",
        "               relevance_score * 0.3 + confidence_score * 0.2)\n",
        "\n",
        "    return {\n",
        "        'grounding_score': round(grounding_score, 3),\n",
        "        'length_score': round(length_score, 3),\n",
        "        'relevance_score': round(relevance_score, 3),\n",
        "        'confidence_score': round(confidence_score, 3),\n",
        "        'overall_answer_score': round(overall, 3),\n",
        "        'quality_label': 'Excellent' if overall > 0.75 else 'Good' if overall > 0.6 else 'Fair'\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3bm-Z4peiZb"
      },
      "source": [
        "# End-to-End RAG Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bvSiG9nfeZ6X"
      },
      "outputs": [],
      "source": [
        "def rag_query(query: str, top_k: int = TOP_K) -> Dict:\n",
        "    \"\"\"\n",
        "    Complete RAG pipeline: retrieve + generate + evaluate.\n",
        "    Simple minimal output matching assignment example.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Step 1: Retrieve\n",
        "    retrieval_result = retrieve_chunks(query, top_k)\n",
        "\n",
        "    print(f\"\\n[RETRIEVAL QUALITY]\")\n",
        "    print(f\"Score: {retrieval_result['retrieval_quality']['overall_retrieval_score']:.3f}\")\n",
        "    print(f\"Label: {retrieval_result['retrieval_quality']['quality_label']}\")\n",
        "    print(f\"Movies: {[m['title'] for m in retrieval_result['metadata']]}\")\n",
        "\n",
        "    # Step 2: Generate answer\n",
        "    generation_result = generate_answer(query, retrieval_result['chunks'])\n",
        "\n",
        "    print(f\"\\n[ANSWER]\")\n",
        "    print(f\"{generation_result['answer']}\")\n",
        "\n",
        "    print(f\"\\n[ANSWER QUALITY]\")\n",
        "    print(f\"Score: {generation_result['answer_quality']['overall_answer_score']:.3f}\")\n",
        "    print(f\"Label: {generation_result['answer_quality']['quality_label']}\")\n",
        "\n",
        "    print(f\"\\n[REASONING]\")\n",
        "    print(f\"{generation_result['reasoning']}\")\n",
        "\n",
        "    # Minimal output format (matching assignment example)\n",
        "    output = {\n",
        "        'answer': generation_result['answer'],\n",
        "        'contexts': retrieval_result['chunks'],\n",
        "        'reasoning': generation_result['reasoning'],\n",
        "        '_metrics': {\n",
        "            'retrieval_score': round(retrieval_result['retrieval_quality']['overall_retrieval_score'], 3),\n",
        "            'answer_score': round(generation_result['answer_quality']['overall_answer_score'], 3),\n",
        "            'system_score': round(\n",
        "                (retrieval_result['retrieval_quality']['overall_retrieval_score'] * 0.4 +\n",
        "                 generation_result['answer_quality']['overall_answer_score'] * 0.6), 3\n",
        "            )\n",
        "        }\n",
        "    }\n",
        "\n",
        "    print(f\"\\n[SYSTEM SCORE]\")\n",
        "    print(f\"Overall: {output['_metrics']['system_score']:.3f}\")\n",
        "\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsOq-SBdepP7"
      },
      "source": [
        "# Test Queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wM7g3zxemj8",
        "outputId": "3cb93582-60fd-4761-fa83-2ffd442634d7"
      },
      "outputs": [],
      "source": [
        "# Example queries\n",
        "test_queries = [\n",
        "    \"Tell me about a movie with a flying saucer\",\n",
        "]\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "for query in test_queries:\n",
        "    result = rag_query(query)\n",
        "    results.append(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMrogakwfrZC"
      },
      "source": [
        "# Output Structured JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqJzgW8uetkY",
        "outputId": "f462c187-17c4-4c19-a2dd-c3e85f093e27"
      },
      "outputs": [],
      "source": [
        "# Save results\n",
        "output_data = {\n",
        "    'system_config': {\n",
        "        'chunk_size': CHUNK_SIZE,\n",
        "        'top_k': TOP_K,\n",
        "        'total_movies': len(df),\n",
        "        'total_chunks': len(chunks_list),\n",
        "        'embedding_method': 'Sentence Transformers',\n",
        "        'vector_store': 'FAISS'\n",
        "    },\n",
        "    'results': [\n",
        "        {\n",
        "            'answer': r['answer'],\n",
        "            'contexts': r['contexts'],\n",
        "            'reasoning': r['reasoning'],\n",
        "            'metrics': r['_metrics']\n",
        "        }\n",
        "        for r in results\n",
        "    ]\n",
        "}\n",
        "\n",
        "with open('rag_results.json', 'w') as f:\n",
        "    json.dump(output_data, f, indent=2)\n",
        "\n",
        "print(\"\\n✓ Results saved to rag_results.json\")\n",
        "\n",
        "# Display sample output\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SAMPLE OUTPUT FORMAT:\")\n",
        "print(\"=\"*70)\n",
        "print(json.dumps(output_data['results'][0], indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHtEj_Dpf_Wl"
      },
      "source": [
        "# Evaluation Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qM2p_cYffvSi",
        "outputId": "a6fb8c72-ca75-4b59-8ae7-095c22a891d6"
      },
      "outputs": [],
      "source": [
        "def print_evaluation_summary(results_list: List[Dict]):\n",
        "    \"\"\"Print summary statistics of all queries.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"EVALUATION SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    retrieval_scores = [r['_metrics']['retrieval_score'] for r in results_list]\n",
        "    answer_scores = [r['_metrics']['answer_score'] for r in results_list]\n",
        "    system_scores = [r['_metrics']['system_score'] for r in results_list]\n",
        "\n",
        "    print(f\"\\nRetrieval Quality:\")\n",
        "    print(f\"  Avg Score: {np.mean(retrieval_scores):.3f}\")\n",
        "    print(f\"  Range: {np.min(retrieval_scores):.3f} - {np.max(retrieval_scores):.3f}\")\n",
        "\n",
        "    print(f\"\\nAnswer Generation Quality:\")\n",
        "    print(f\"  Avg Score: {np.mean(answer_scores):.3f}\")\n",
        "    print(f\"  Range: {np.min(answer_scores):.3f} - {np.max(answer_scores):.3f}\")\n",
        "\n",
        "    print(f\"\\nOverall RAG System:\")\n",
        "    print(f\"  Avg Score: {np.mean(system_scores):.3f}\")\n",
        "    print(f\"  Range: {np.min(system_scores):.3f} - {np.max(system_scores):.3f}\")\n",
        "\n",
        "    print(f\"\\nQueries Tested: {len(results_list)}\")\n",
        "\n",
        "print_evaluation_summary(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EAHTf8EgIdF"
      },
      "source": [
        "# Interactive Query Function (for notebook testing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "mN6k3TlDgCAR"
      },
      "outputs": [],
      "source": [
        "def interactive_rag(query: str) -> None:\n",
        "    \"\"\"Simple function to test queries in notebook.\"\"\"\n",
        "    result = rag_query(query)\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"JSON OUTPUT:\")\n",
        "    print(\"=\"*70)\n",
        "    print(json.dumps(result, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFdKK-TigLoL",
        "outputId": "da8bd2e5-1200-4e8a-ac56-42297b23e059"
      },
      "outputs": [],
      "source": [
        "interactive_rag(\"tell me about Alice Chicoy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "tmma5GUqgaCQ"
      },
      "outputs": [],
      "source": [
        "def rag_pipeline(query: str):\n",
        "    results = retrieve_chunks(query)\n",
        "    answer = generate_answer(query, results[\"chunks\"])\n",
        "    return {\n",
        "        \"answer\": answer['answer'],\n",
        "        \"contexts\": results[\"chunks\"],\n",
        "        \"reasoning\": answer['reasoning']\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"Which movie has an AI system named HAL 9000?\"\n",
        "output = rag_pipeline(query)\n",
        "print(json.dumps(output, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01a9f9768b5d4601845d002fc42d7c29": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15d66ce6f9034bb0b195fee9b3c4a263": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0946a9a2a8840a99eaba297060f78ce",
            "placeholder": "​",
            "style": "IPY_MODEL_d37bba5697cb4752b002a6701e9dc627",
            "value": " 30/30 [01:52&lt;00:00,  1.06s/it]"
          }
        },
        "2aed01c77a3646dd922c5e30806dd0ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33b7aab67ae6444e9736c3af3e0e2353": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01a9f9768b5d4601845d002fc42d7c29",
            "placeholder": "​",
            "style": "IPY_MODEL_cc749e91e9c340a1a1f0a42bef7ff8f9",
            "value": "Batches: 100%"
          }
        },
        "35c38884c3bc418999307f191d7e9384": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "875bf8d032ee4a3ba77ca0dbba689181": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2aed01c77a3646dd922c5e30806dd0ff",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ad3069c979834ebcaa578fb1909a29b0",
            "value": 30
          }
        },
        "a0946a9a2a8840a99eaba297060f78ce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad3069c979834ebcaa578fb1909a29b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc749e91e9c340a1a1f0a42bef7ff8f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d37bba5697cb4752b002a6701e9dc627": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eed46a2a00b048658931515f7200ba57": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_33b7aab67ae6444e9736c3af3e0e2353",
              "IPY_MODEL_875bf8d032ee4a3ba77ca0dbba689181",
              "IPY_MODEL_15d66ce6f9034bb0b195fee9b3c4a263"
            ],
            "layout": "IPY_MODEL_35c38884c3bc418999307f191d7e9384"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
